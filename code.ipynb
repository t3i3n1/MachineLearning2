{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Data Split\n",
    "Use train_dataset and eval_dataset as train / test sets\n",
    "\n",
    "'''\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import ConcatDataset, Subset\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "import numpy as np\n",
    "    \n",
    "# For convenience, show image at index in dataset\n",
    "def show_image(dataset, index):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(dataset[index][0][0], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "def get_datasets(split='balanced', save=False):\n",
    "    download_folder = './data'\n",
    "  \n",
    "    transform = Compose([ToTensor()])\n",
    "\n",
    "    dataset = ConcatDataset([EMNIST(root=download_folder, split=split, download=True, train=False, transform=transform),\n",
    "                           EMNIST(root=download_folder, split=split, download=True, train=True, transform=transform)])\n",
    "    \n",
    "  # Ignore the code below with argument 'save'\n",
    "    if save:\n",
    "        random_seed = 4211 # do not change\n",
    "        n_samples = len(dataset)\n",
    "        eval_size = 0.2\n",
    "        indices = list(range(n_samples))\n",
    "        split = int(np.floor(eval_size * n_samples))\n",
    "\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_indices, eval_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # cut to half\n",
    "        train_indices = train_indices[:len(train_indices)//2]\n",
    "        eval_indices = eval_indices[:len(eval_indices)//2]\n",
    "\n",
    "        np.savez('train_test_split.npz', train=train_indices, test=eval_indices)\n",
    "  \n",
    "  # just use save=False for students\n",
    "  # load train test split indices\n",
    "    else:\n",
    "        with np.load('./train_test_split.npz') as f:\n",
    "            train_indices = f['train']\n",
    "            eval_indices = f['test']\n",
    "\n",
    "    train_dataset = Subset(dataset, indices=train_indices)\n",
    "    eval_dataset = Subset(dataset, indices=eval_indices)\n",
    "  \n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "# TODO\n",
    "# 1. build your own CNN classifier with the given structure. DO NOT COPY OR USE ANY TRICK\n",
    "# 2. load pretrained encoder from 'pretrained_encoder.pt' and build a CNN classifier on top of the encoder\n",
    "# 3. load pretrained encoder from 'pretrained_encoder.pt' and build a Convolutional Autoencoder on top of the encoder (just need to implement decoder)\n",
    "# *** Note that all the above tasks include implementation, training, analyzing, and reporting\n",
    "\n",
    "# example main code\n",
    "# each img has size (1, 28, 28) and each label is in {0, ..., 46}, a total of 47 classes\n",
    "#if __name__=='__main__':\n",
    "  #train_ds, eval_ds = get_datasets()\n",
    "  \n",
    "  #img_index = 10\n",
    "  #show_image(train_ds, img_index)\n",
    "  #show_image(eval_ds, img_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to print mean and S.D of optimal loss and top 1 accuracy and top 3 accuracy\n",
    "from statistics import *\n",
    "def print_stat(optimal_loss,optimal_acc1,optimal_acc3):\n",
    "    stacked = torch.stack(optimal_loss,dim=0)\n",
    "    print('mean of optimal loss: ', stacked.mean().item())\n",
    "    print('SD of optimal loss: ', stacked.std().item())\n",
    "    print('mean of optimal top1 accuracy: ', mean(optimal_acc1))\n",
    "    print('SD of optimal top1 accuracy: ', pstdev(optimal_acc1))\n",
    "    print('mean of optimal top3 accuracy: ', mean(optimal_acc3))\n",
    "    print('SD of optimal top3 accuracy: ', pstdev(optimal_acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "def train(model, loaders, optimizer, n_epochs, hyperpara,writer=None):\n",
    "    def run_epoch(train_or_eval):\n",
    "        epoch_loss = 0.\n",
    "        epoch_acc1 = 0.\n",
    "        epoch_acc3 = 0.\n",
    "        for i, (images, labels) in enumerate(loaders[train_or_eval], 1):\n",
    "            if train_or_eval == 'train':\n",
    "                optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            batch_loss = model.loss(logits, labels)\n",
    "            batch_acc1 = model.top1_accuracy(logits, labels)\n",
    "            batch_acc3 = model.top3_accuracy(logits,labels)\n",
    "            if train_or_eval == 'eval':\n",
    "                loss_list.append(batch_loss)\n",
    "                top1_list.append(batch_acc1)\n",
    "                top3_list.append(batch_acc3)\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_acc1 += batch_acc1\n",
    "            epoch_acc3 += batch_acc3\n",
    "            if train_or_eval == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_step = len(loaders['train'])\n",
    "                if (i + 1) % 100 == 0 or (i+1) == total_step or (i+1) == total_step:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Top 1 Accuracy: {:.2f}%, Top 3 Accuracy: {:.2f}%'\n",
    "                          .format(epoch + 1, n_epochs, i + 1, total_step, batch_loss,batch_acc1,batch_acc3))\n",
    "#             if writer is not None:\n",
    "#                 if len(images.size()) == 2: # when it is flattened, reshape it\n",
    "#                     images = images.view(-1, 1, 28, 28)\n",
    "#                 img_grid = make_grid(images)\n",
    "#                 writer.add_image('%s/images_%s' % (model.__class__.__name__,epoch), img_grid, epoch)\n",
    "        epoch_loss /= i\n",
    "        epoch_acc1 /= i\n",
    "        epoch_acc3 /= i\n",
    "        losses[train_or_eval] = epoch_loss\n",
    "        accs1[train_or_eval] = epoch_acc1\n",
    "        accs3[train_or_eval] = epoch_acc3\n",
    "        if writer is None:\n",
    "            print('epoch {} [{}] Loss: {:.4f} Top 1 Accuracy: {:.2f}% Top 3 Accuracy: {:.2f}%'.format(epoch, train_or_eval, epoch_loss, epoch_acc1, epoch_acc3))\n",
    "        elif train_or_eval == 'eval':\n",
    "            writer.add_scalars('%s_loss_%s_%s' % (model.__class__.__name__,hyperpara['opt'],hyperpara['lr']), # CnnClassifier or FcClassifier\n",
    "                               {'train': losses['train'], \n",
    "                                'eval': losses['eval']}, \n",
    "                              epoch)\n",
    "            writer.add_scalars('%s_top1_accuracy_%s_%s' % (model.__class__.__name__,hyperpara['opt'],hyperpara['lr']), # CnnClassifier or FcClassifier\n",
    "                         {'train': accs1['train'], \n",
    "                          'eval': accs1['eval']}, \n",
    "                          epoch)\n",
    "            writer.add_scalars('%s_top3_accuracy_%s_%s' % (model.__class__.__name__,hyperpara['opt'],hyperpara['lr']), # CnnClassifier or FcClassifier\n",
    "                         {'train': accs3['train'], \n",
    "                          'eval': accs3['eval']}, \n",
    "                          epoch)\n",
    "    # end of run_epoch\n",
    "    losses = dict()\n",
    "    accs1 = dict()\n",
    "    accs3 = dict()\n",
    "    loss_list = []\n",
    "    top1_list = []\n",
    "    top3_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')\n",
    "    return loss_list,top1_list,top3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "def train_cae(model, loaders, optimizer, n_epochs, hyperpara,writer=None):\n",
    "    def run_epoch(train_or_eval):\n",
    "        epoch_loss = 0.\n",
    "        for i, (images, labels) in enumerate(loaders[train_or_eval], 1):\n",
    "            if train_or_eval == 'train':\n",
    "                optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            batch_loss = model.loss(logits, images)\n",
    "            if train_or_eval == 'eval':\n",
    "                loss_list.append(batch_loss)\n",
    "            epoch_loss += batch_loss.item()\n",
    "            if train_or_eval == 'train':\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "                total_step = len(loaders['train'])\n",
    "                if (i + 1) % 100 == 0 or (i+1) == total_step or (i+1) == total_step:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                          .format(epoch + 1, n_epochs, i + 1, total_step, batch_loss))\n",
    "            if writer is not None:\n",
    "                if len(images.size()) == 2: # when it is flattened, reshape it\n",
    "                    images = images.view(-1, 1, 28, 28)\n",
    "                #img_grid = make_grid(images)\n",
    "                img_grid = make_grid(logits) # reconstructed image\n",
    "                writer.add_image('%s/images_%s_%s_%s' % (model.__class__.__name__,hyperpara['opt'],hyperpara['lr'],epoch), img_grid, epoch)\n",
    "        epoch_loss /= i\n",
    "        losses[train_or_eval] = epoch_loss\n",
    "        print('epoch {} [{}] Loss: {:.4f} '.format(epoch, train_or_eval, epoch_loss))\n",
    "        if writer is not None and train_or_eval == 'eval':\n",
    "            writer.add_scalars('%s_loss_%s_%s' % (model.__class__.__name__,hyperpara['opt'],hyperpara['lr']), # CnnClassifier or FcClassifier\n",
    "                               {'train': losses['train'], \n",
    "                                'eval': losses['eval']}, \n",
    "                              epoch)\n",
    "    # end of run_epoch\n",
    "    losses = dict()\n",
    "    loss_list = []\n",
    "    for epoch in range(n_epochs):\n",
    "        run_epoch('train')\n",
    "        run_epoch('eval')\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition training data randomly into 80% training, 20% validation\n",
    "from torch.optim import Adam, SGD\n",
    "def holdout(model_name,dataset,hyperpara_list,writer=None):\n",
    "    train_ds,validation_ds = torch.utils.data.random_split(dataset,[int(0.8*len(dataset)),int(0.2*len(dataset))])\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(dataset=train_ds, batch_size=32, shuffle=True),\n",
    "        'eval': DataLoader(dataset=validation_ds, batch_size=32, shuffle=False) \n",
    "    }\n",
    "    # train all the models\n",
    "    loss_list = []\n",
    "    top1_acc_list = []\n",
    "#     top3_acc_list = []\n",
    "    for i in range(0,len(hyperpara_list)):\n",
    "        if model_name == 'cnn':\n",
    "            model = CnnClassifier(hyperpara_list[i]['H'])\n",
    "        elif model_name == 'cnn_pretrained':\n",
    "            model = FcClassifier(hyperpara_list[i]['H'])\n",
    "        if (hyperpara_list[i]['opt'] == 'ADAM'):\n",
    "            optimizer = Adam(model.parameters(), lr=hyperpara_list[i]['lr'])\n",
    "        else:\n",
    "            optimizer = SGD(model.parameters(), lr=hyperpara_list[i]['lr'])\n",
    "        loss,top1_acc,top3_acc = train(model,dataloaders,optimizer,3,hyperpara_list[i],writer)\n",
    "        loss_list.append(loss)\n",
    "        top1_acc_list.append(top1_acc)\n",
    "#         top3_acc_list.append(top3_acc)\n",
    "\n",
    "    # print the optimal results\n",
    "    optimal_acc1 = [] # top 1 acc for criteria\n",
    "#     optimal_acc3 = []\n",
    "    for i in range(0,len(hyperpara_list)):\n",
    "        print('optimal loss for hyperparameter setting {}: {}'.format(i+1,min(loss_list[i])))\n",
    "        print('optimal top1 accuracy for hyperparameter setting {}: {}%'.format(i+1,max(top1_acc_list[i])))\n",
    "#         optimal_loss.append(min(loss_list[i]))\n",
    "        optimal_acc1.append(max(top1_acc_list[i]))\n",
    "#         optimal_acc3.append(max(top3_acc_list[i]))\n",
    "    # choose the best accuracy model\n",
    "    max_index = optimal_acc1.index(max(optimal_acc1))\n",
    "    return hyperpara_list[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class CnnClassifier(nn.Module):\n",
    "  # n_hidden: number of units at the last fc layer\n",
    "    def __init__(self,n_hidden):\n",
    "        super(CnnClassifier, self).__init__()\n",
    "        #CNN encoder\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=2, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(kernel_size=3,stride=1,padding=0),\n",
    "          nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "        # CNN predictor\n",
    "        # linear layers transforms flattened image features into logits before the softmax layer\n",
    "        self.linear = nn.Sequential(\n",
    "          nn.Linear(32, n_hidden),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden, 47) \n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='sum') \n",
    "        \n",
    "    def forward(self, in_data):\n",
    "        img_features = self.cnn_layers(in_data).view(in_data.size(0), -1) \n",
    "        logits = self.linear(img_features)\n",
    "        return logits  \n",
    "    \n",
    "    def loss(self, logits, labels):\n",
    "        #preds = self.softmax(logits) \n",
    "        return self.loss_function(logits, labels) / logits.size(0) \n",
    "    \n",
    "    def top1_accuracy(self, logits, labels):\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        return (correct / total) * 100\n",
    "    \n",
    "    def top3_accuracy(self, logits, labels):\n",
    "        total = labels.size(0)\n",
    "        _, pred = torch.topk(logits,3,dim=1)\n",
    "        stack_labels = torch.stack([labels,labels,labels],1)\n",
    "        correct_tensor = pred.eq(stack_labels)\n",
    "        correct = torch.sum(correct_tensor).item()\n",
    "        return (correct / total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FcClassifier(nn.Module):\n",
    "  # n_hidden: number of units at the last fc layer\n",
    "    def __init__(self,n_hidden):\n",
    "        super(FcClassifier, self).__init__()\n",
    "        \n",
    "        saved = torch.load('pretrained_encoder.pt')\n",
    "        cnn_layers = saved['model'][0]\n",
    "        self.cnn_layers = cnn_layers\n",
    "        \n",
    "        # CNN predictor\n",
    "        # linear layers transforms flattened image features into logits before the softmax layer\n",
    "        self.linear = nn.Sequential(\n",
    "          nn.Linear(32, n_hidden),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(n_hidden, 47) \n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='sum') \n",
    "        \n",
    "    def forward(self, in_data):\n",
    "        img_features = self.cnn_layers(in_data).view(in_data.size(0), -1) \n",
    "        logits = self.linear(img_features)\n",
    "        return logits  \n",
    "    \n",
    "    def loss(self, logits, labels):\n",
    "        #preds = self.softmax(logits) \n",
    "        return self.loss_function(logits, labels) / logits.size(0) \n",
    "    \n",
    "    def top1_accuracy(self, logits, labels):\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        return (correct / total) * 100\n",
    "    \n",
    "    def top3_accuracy(self, logits, labels):\n",
    "        total = labels.size(0)\n",
    "        _, pred = torch.topk(logits,3,dim=1)\n",
    "        stack_labels = torch.stack([labels,labels,labels],1)\n",
    "        correct_tensor = pred.eq(stack_labels)\n",
    "        correct = torch.sum(correct_tensor).item()\n",
    "        return (correct / total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAE decoder\n",
    "class Decoder(nn.Module):\n",
    "  # n_hidden: number of units at the last fc layer\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        saved = torch.load('pretrained_encoder.pt')\n",
    "        cnn_layers = saved['model'][0]\n",
    "        self.cnn_layers = cnn_layers\n",
    "        \n",
    "        self.decoder_layers = nn.Sequential(\n",
    "          nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3, stride=2, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=0),\n",
    "          nn.ReLU(),\n",
    "          nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=4, stride=2, padding=0),\n",
    "          nn.Sigmoid()\n",
    "        )\n",
    "        self.loss_function = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    def forward(self, in_data):\n",
    "        img_features = self.cnn_layers(in_data)\n",
    "        #print('img',img_features.size())\n",
    "        logits = self.decoder_layers(img_features)\n",
    "        #print('logits',logits.size())\n",
    "        return logits  \n",
    "    \n",
    "    def loss(self, logits, images):\n",
    "        #preds = self.softmax(logits) \n",
    "        return self.loss_function(logits, images) / logits.size(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_cae(dataset,hyperpara_list,writer=None):\n",
    "    train_ds,validation_ds = torch.utils.data.random_split(dataset,[int(0.8*len(dataset)),int(0.2*len(dataset))])\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(dataset=train_ds, batch_size=32, shuffle=True),\n",
    "        'eval': DataLoader(dataset=validation_ds, batch_size=32, shuffle=False) \n",
    "    }\n",
    "    # train all the models\n",
    "    loss_list = []\n",
    "    for i in range(0,len(hyperpara_list)):\n",
    "        model = Decoder()\n",
    "        if (hyperpara_list[i]['opt'] == 'ADAM'):\n",
    "            optimizer = Adam(model.parameters(), lr=hyperpara_list[i]['lr'])\n",
    "        else:\n",
    "            optimizer = SGD(model.parameters(), lr=hyperpara_list[i]['lr'])\n",
    "        loss = train_cae(model,dataloaders,optimizer,3,hyperpara_list[i],writer)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "\n",
    "    # print the optimal results\n",
    "    optimal_loss = [] # top 1 acc for criteria\n",
    "    for i in range(0,len(hyperpara_list)):\n",
    "        print('optimal loss for hyperparameter setting {}: {}'.format(i+1,min(loss_list[i])))\n",
    "        optimal_loss.append(min(loss_list[i]))\n",
    "    # choose the best accuracy model\n",
    "    min_index = optimal_loss.index(min(optimal_loss))\n",
    "    return hyperpara_list[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "#from torch.optim import Adam, SGD\n",
    "train_ds, test_ds = get_datasets()\n",
    "dataloaders = {\n",
    "    'train': DataLoader(dataset=train_ds, batch_size=32, shuffle=True),\n",
    "    'eval': DataLoader(dataset=test_ds, batch_size=32, shuffle=False)\n",
    "}\n",
    "writer = SummaryWriter('log')\n",
    "print('=========running holdout validation for cnn============')\n",
    "cnn_hyper_list = [{'H' : 32, 'opt' : 'ADAM', 'lr' : 0.001},\n",
    "                  {'H' : 32, 'opt' : 'SGD', 'lr' : 0.1},\n",
    "                  {'H' : 32, 'opt' : 'SGD', 'lr' : 0.01},\n",
    "                  {'H' : 64, 'opt' : 'ADAM', 'lr' : 0.001},\n",
    "                  {'H' : 64, 'opt' : 'SGD', 'lr' : 0.1},\n",
    "                  {'H' : 64, 'opt' : 'SGD', 'lr' : 0.01}    \n",
    "                  ]\n",
    "cnn_best_hyperpara = holdout('cnn',train_ds,cnn_hyper_list)\n",
    "print('cnn hyperparameter chosen: ',cnn_best_hyperpara)\n",
    "print('=========finish holdout validation==============')\n",
    "# #After validation, learn from scratch using the best parameter\n",
    "print('============training model from scratch with optimal hyperparameters===========')\n",
    "loss_list = []\n",
    "top1_acc_list = []\n",
    "top3_acc_list = []\n",
    "optimal_loss = []\n",
    "optimal_acc1 = []\n",
    "optimal_acc3 = []\n",
    "best_model = CnnClassifier(cnn_best_hyperpara['H'])\n",
    "if (cnn_best_hyperpara['opt'] == 'ADAM'):\n",
    "    optimizer = Adam(best_model.parameters(), lr=cnn_best_hyperpara['lr'])\n",
    "else:\n",
    "    optimizer = SGD(best_model.parameters(), lr=cnn_best_hyperpara['lr'])\n",
    "# train the model 5 times\n",
    "t0 = time.time()\n",
    "for i in range(5):  \n",
    "    loss,top1_acc,top3_acc = train(best_model,dataloaders, optimizer, 3, cnn_best_hyperpara,writer)\n",
    "    loss_list.append(loss)\n",
    "    top1_acc_list.append(top1_acc)\n",
    "    top3_acc_list.append(top3_acc)\n",
    "    optimal_loss.append(min(loss_list[i]))\n",
    "    optimal_acc1.append(max(top1_acc_list[i]))\n",
    "    optimal_acc3.append(max(top3_acc_list[i]))\n",
    "print('training time: {} seconds'.format(time.time() - t0))\n",
    "print_stat(optimal_loss,optimal_acc1,optimal_acc3)\n",
    "print('=============finish training=======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=============learning from pre-trained Encoder==================')\n",
    "cnn_best_hyperpara = holdout('cnn_pretrained',train_ds,cnn_hyper_list)\n",
    "print('cnn hyperparameter chosen: ',cnn_best_hyperpara)\n",
    "print('=========finish holdout validation==============')\n",
    "print('============training model from scratch with optimal hyperparameters===========')\n",
    "loss_list = []\n",
    "top1_acc_list = []\n",
    "top3_acc_list = []\n",
    "optimal_loss = []\n",
    "optimal_acc1 = []\n",
    "optimal_acc3 = []\n",
    "best_model = FcClassifier(cnn_best_hyperpara['H'])\n",
    "if (cnn_best_hyperpara['opt'] == 'ADAM'):\n",
    "    optimizer = Adam(best_model.parameters(), lr=cnn_best_hyperpara['lr'])\n",
    "else:\n",
    "    optimizer = SGD(best_model.parameters(), lr=cnn_best_hyperpara['lr'])\n",
    "# train the model 5 times\n",
    "t0 = time.time()\n",
    "for i in range(5):  \n",
    "    loss,top1_acc,top3_acc = train(best_model,dataloaders, optimizer, 3, cnn_best_hyperpara,writer)\n",
    "    loss_list.append(loss)\n",
    "    top1_acc_list.append(top1_acc)\n",
    "    top3_acc_list.append(top3_acc)\n",
    "    optimal_loss.append(min(loss_list[i]))\n",
    "    optimal_acc1.append(max(top1_acc_list[i]))\n",
    "    optimal_acc3.append(max(top3_acc_list[i]))\n",
    "print('training time: {} seconds'.format(time.time() - t0))\n",
    "print_stat(optimal_loss,optimal_acc1,optimal_acc3)\n",
    "print('==============finish training====================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========running holdout validation for cae============\n",
      "============training model from scratch with optimal hyperparameters===========\n",
      "Epoch [1/3], Step [100/1645], Loss: 66.2611\n",
      "Epoch [1/3], Step [200/1645], Loss: 67.4874\n",
      "Epoch [1/3], Step [300/1645], Loss: 69.8884\n",
      "Epoch [1/3], Step [400/1645], Loss: 66.8353\n",
      "Epoch [1/3], Step [500/1645], Loss: 68.0339\n",
      "Epoch [1/3], Step [600/1645], Loss: 70.9821\n",
      "Epoch [1/3], Step [700/1645], Loss: 64.2436\n",
      "Epoch [1/3], Step [800/1645], Loss: 67.1752\n",
      "Epoch [1/3], Step [900/1645], Loss: 69.2359\n",
      "Epoch [1/3], Step [1000/1645], Loss: 68.4411\n",
      "Epoch [1/3], Step [1100/1645], Loss: 65.3218\n",
      "Epoch [1/3], Step [1200/1645], Loss: 64.5302\n",
      "Epoch [1/3], Step [1300/1645], Loss: 59.7491\n",
      "Epoch [1/3], Step [1400/1645], Loss: 59.6748\n",
      "Epoch [1/3], Step [1500/1645], Loss: 60.1718\n",
      "Epoch [1/3], Step [1600/1645], Loss: 46.4357\n",
      "Epoch [1/3], Step [1645/1645], Loss: 52.3768\n",
      "epoch 0 [train] Loss: 66.0263 \n",
      "epoch 0 [eval] Loss: 48.2940 \n",
      "Epoch [2/3], Step [100/1645], Loss: 48.1434\n",
      "Epoch [2/3], Step [200/1645], Loss: 45.4923\n",
      "Epoch [2/3], Step [300/1645], Loss: 39.5051\n",
      "Epoch [2/3], Step [400/1645], Loss: 46.2952\n",
      "Epoch [2/3], Step [500/1645], Loss: 40.8865\n",
      "Epoch [2/3], Step [600/1645], Loss: 37.1091\n",
      "Epoch [2/3], Step [700/1645], Loss: 38.4257\n",
      "Epoch [2/3], Step [800/1645], Loss: 32.4005\n",
      "Epoch [2/3], Step [900/1645], Loss: 34.3020\n",
      "Epoch [2/3], Step [1000/1645], Loss: 32.4198\n",
      "Epoch [2/3], Step [1100/1645], Loss: 31.4859\n",
      "Epoch [2/3], Step [1200/1645], Loss: 28.1832\n",
      "Epoch [2/3], Step [1300/1645], Loss: 31.1902\n",
      "Epoch [2/3], Step [1400/1645], Loss: 30.5626\n",
      "Epoch [2/3], Step [1500/1645], Loss: 29.4374\n",
      "Epoch [2/3], Step [1600/1645], Loss: 26.6400\n",
      "Epoch [2/3], Step [1645/1645], Loss: 30.1560\n",
      "epoch 1 [train] Loss: 35.3203 \n",
      "epoch 1 [eval] Loss: 27.7784 \n",
      "Epoch [3/3], Step [100/1645], Loss: 27.3415\n",
      "Epoch [3/3], Step [200/1645], Loss: 25.5456\n",
      "Epoch [3/3], Step [300/1645], Loss: 27.6507\n",
      "Epoch [3/3], Step [400/1645], Loss: 22.9480\n",
      "Epoch [3/3], Step [500/1645], Loss: 23.0608\n",
      "Epoch [3/3], Step [600/1645], Loss: 27.4928\n",
      "Epoch [3/3], Step [700/1645], Loss: 27.9445\n",
      "Epoch [3/3], Step [800/1645], Loss: 22.7786\n"
     ]
    }
   ],
   "source": [
    "print('=========running holdout validation for cae============')\n",
    "cae_hyper_list = [{'opt' : 'ADAM', 'lr' : 0.001},\n",
    "                  {'opt' : 'SGD', 'lr' : 0.1},\n",
    "                  {'opt' : 'SGD', 'lr' : 0.01}\n",
    "                  ]\n",
    "cae_best_hyperpara = holdout_cae(train_ds,cae_hyper_list,writer) #reconstruct images\n",
    "print('cae hyperparameter chosen: ',cae_best_hyperpara)\n",
    "print('===========finish holdout validation=====================')\n",
    "print('============training model from scratch with optimal hyperparameters===========')\n",
    "loss_list = []\n",
    "optimal_loss = []\n",
    "best_cae = Decoder()\n",
    "if (cae_best_hyperpara['opt'] == 'ADAM'):\n",
    "    optimizer = Adam(best_cae.parameters(), lr=cae_best_hyperpara['lr'])\n",
    "else:\n",
    "    optimizer = SGD(best_cae.parameters(), lr=cae_best_hyperpara['lr'])\n",
    "optimizer = SGD(best_cae.parameters(), lr=0.01) # delete this\n",
    "loss_cae = train_cae(best_cae,dataloaders, optimizer, 3, cae_best_hyperpara,writer)\n",
    "loss_list.append(loss_cae)\n",
    "optimal_loss.append(min(loss_list))\n",
    "print('optimal loss: ', min(optimal_loss))\n",
    "print('=============finish training=======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
